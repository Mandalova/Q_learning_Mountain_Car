{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8910a240-93a3-411e-b6f2-4b7470616f57",
   "metadata": {},
   "source": [
    "# Mountain Car - Q учење"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d21e536-72f1-4e6a-891d-ed0dbc97015f",
   "metadata": {},
   "source": [
    "## Проблем"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb5411b-7e88-4e84-849d-374dfde175ba",
   "metadata": {},
   "source": [
    "Во оваа тетратка се решава проблемот на Автомобил на планински пат(Mountain Car) со користење на алгоритмот Q учење. Целта е агентот-количката, која се наоѓа помеѓу 2 рида, да научи да се искачи на десниот рид и да стигне до знаменцето кое се наоѓа на неговиот врв. Повеќе детали за проблемот може да се најдат на [официјалната страна на библиотеката Gymnasium.](https://gymnasium.farama.org/environments/classic_control/mountain_car/)\n",
    "\n",
    "\n",
    "![Mountain_Car](images/mountain_car.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12561875-ffc1-43ea-b9d3-ece1d8b703ba",
   "metadata": {},
   "source": [
    "### Што претставува **Q-учење**? \n",
    "([Книга AI Crash Course, Chapter 7](https://e-kursevi.feit.ukim.edu.mk/pluginfile.php/109092/mod_resource/content/1/Crash%20Course%20-%20Reinforcement%20Learning%2C%20Deep%20Learning%2C%20and%20Artificial%20Intelligence%20with%20Python%20%282019%29.pdf))\n",
    "1. Q-учењето е модел на учење со поттикнување (Reinforcement Learning);\n",
    "2. Q-учењето функционира според принципот на влезови (состојби) и излези (акции);\n",
    "3. Q-учењето се изведува во унапред дефинирана околина, која ги содржи состојбите (влезовите), акциите (излезите) и наградите;\n",
    "4. Q-учењето се моделира со Марков процес на одлучување (Markov Decision Process);\n",
    "5. Q-учењето користи режим на тренирање, при што се учат параметри наречени Q-вредности, и режим на инференција;\n",
    "6. Постојат конечен број на состојби (нема бесконечен број можни влезови);\n",
    "7. Постојат конечен број на акции (може да се изведат само одреден број на акции).\n",
    "\n",
    "\n",
    "([Q-learning, од Википедија](https://en.wikipedia.org/wiki/Q-learning))\n",
    "\n",
    "Q-учењето е алгоритам за учење со поттикнување кој тренира агент да доделува вредности на можните акции врз основа на моменталната состојба, без да му биде потребен модел на околината (model-free). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59f4467-d1f4-4193-8fef-3486d65443a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import statistics\n",
    "from collections import defaultdict, deque\n",
    "import threading\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9e5acb-bbc9-4139-b2e8-370b2f7f5cff",
   "metadata": {},
   "source": [
    "**ALPHA** и **GAMMA** се хиперпараметри кои се користат за пресметка на Белмановата равенка. ALPHA е стапка на учење(0,1), повисока вредност на ALPHA послабо учи алгоритмот. GAMMA(0,1) кажува колку се важни акциите што се преземаат во иднина. **EPSILON**(0,1) е хиперпараметар кој контролира колкав дел од акциите ќе бидат случајно избрани, а колкав дел ќе бидат базирани на наученото знаење."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0098d452-4431-49f0-a312-0f2ce12fc63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.3 \n",
    "GAMMA = 0.95 \n",
    "EPSILON = 0.22  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02463fe-dcc7-4dd5-946f-97375b6a2a10",
   "metadata": {},
   "source": [
    "## Состојбениот вектор на овој проблем е ($x$, $\\dot{x}$),  односно позиција и брзина на агентот-количката"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90bab0e-0f8d-441b-bbe5-8a40c4a4d1ed",
   "metadata": {},
   "source": [
    "#### Q-учењето е алгоритам кој работи со дискретни вредности на состојбениот вектор. Поради тоа, континуалните вредности на позицијата и брзината на агентот треба да се дискретизираат.Ова се реализира со помош на функцијата np.linspace, која како влез прима: минимална вредност и максимална вредност на состојбената променлива како и број на поделци (кошнички). Во овој пример се користат 100 кошнички, со што се овозможува соодветна дискретизација на континуалниот простор на состојби."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bfba529-4a9e-4701-a1be-ff3897c6d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BINS_X = 100\n",
    "N_BINS_X_DOT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd847f9-31ed-4a0b-be38-9af87b81e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_low, x_high = -1.2, 0.6  \n",
    "x_dot_low, x_dot_high = -0.07, 0.07 \n",
    "\n",
    "bins_x = np.linspace(x_low, x_high, num=N_BINS_X)\n",
    "bins_x_dot = np.linspace(x_dot_low, x_dot_high, num=N_BINS_X_DOT)\n",
    "\n",
    "BINS = [bins_x, bins_x_dot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3158b1f-bffa-4785-81e1-54764947559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_from_observation(observation):\n",
    "    \"\"\"\n",
    "    Го дискретизира набљудувањето за да го претвори во дискретна состојба погодна за Q-учење.\n",
    "\n",
    "     Влезни аргументи:\n",
    "        observation (np.ndarray): Влезен вектор што ја претставува моменталната состојба на агентот,\n",
    "                                  содржи континуални вредности (позиција и брзина).\n",
    "\n",
    "    Излез:\n",
    "        tuple: Дискретизирана состојба изразена како торка, каде секој елемент покажува \n",
    "               во која кошничка се наоѓа соодветната континуална вредност.\n",
    "    \"\"\"\n",
    "    return tuple([np.digitize(var, bins) for var, bins in zip(observation, BINS)])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74923284-6fc5-4ef3-ae8c-cb403f605df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(env, i_state, Q, episode):    \n",
    "    \"\"\"\n",
    "    Ја дефинира полисата според која агентот избира акција, користејќи ε-greedy стратегија.\n",
    "\n",
    "    Влезни аргументи:\n",
    "        env (gym.Env): Околината во која агентот се наоѓа. Се користи за избор на случајна акција.\n",
    "        i_state (tuple): Дискретизирана моментална состојба на агентот.\n",
    "        Q (dict): Q-табела што содржи Q-вредности за секоја состојба и можни акции.\n",
    "        episode (int): Тековна епизода од тренирањето, може да се користи за адаптација на ε.\n",
    "\n",
    "    Излез:\n",
    "        int: Акција што агентот треба да ја изврши, или најдобрата позната акција (експлоатација),\n",
    "             или случајна акција (експлорација), во зависност од ε.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < EPSILON or not Q[i_state]:\n",
    "        return env.action_space.sample()\n",
    "    return max(Q[i_state], key=Q[i_state].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78894191-3087-489c-8373-dbf58f82f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bellman equation\n",
    "def update_q(Q, i_state, i_next_state, action, reward):\n",
    "    \"\"\"\n",
    "    Ја ажурира Q-табелата според Белмановата равенка за Q-учење.\n",
    "\n",
    "    Влезни аргументи:\n",
    "        Q (dict): Q-табела што ги содржи Q-вредностите за сите состојби и акции.\n",
    "        i_state (tuple): Моменталната (дискретизирана) состојба на агентот.\n",
    "        i_next_state (tuple): Наредната состојба добиена по извршување на акцијата.\n",
    "        action (int): Акцијата што била избрана во моменталната состојба.\n",
    "        reward (float): Наградата добиена по извршувањето на акцијата.\n",
    "\n",
    "    Алгоритам:\n",
    "        - Прво се пресметува највисоката Q-вредност за наредната состојба.\n",
    "        - Потоа се пресметува Темпоралната Разлика (TD error).\n",
    "        - Конечно, Q-вредноста за дадената состојба и акција се ажурира според TD error и факторот на учење (ALPHA).\n",
    "    \"\"\"\n",
    "    max_q_value = max(Q[i_next_state].values(), default=0)\n",
    "    TD = reward + GAMMA * max_q_value - Q[i_state][action]\n",
    "    Q[i_state][action] += ALPHA * TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3532cd86-3f4d-4476-8ba9-e24fcdb7e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dashboard(training, step_log=1000):\n",
    "    h_rewards = deque(maxlen=step_log)\n",
    "\n",
    "    dashboard.STOP_TRAINING = False\n",
    "    bt_stop = widgets.Button(description='Stop Training', button_style='danger')\n",
    "    def f_stop(bt_stop):\n",
    "        dashboard.STOP_TRAINING = True\n",
    "    bt_stop.on_click(f_stop)\n",
    "\n",
    "    fig = go.FigureWidget()\n",
    "    fig.add_scatter(x=[], y=[], mode='markers+lines')\n",
    "    fig.layout.yaxis.rangemode = 'tozero'\n",
    "    fig.layout.title = 'Dashboard for training RL agents'\n",
    "    fig.layout.xaxis.title = 'Episode'\n",
    "    fig.layout.yaxis.title = 'Reward'\n",
    "\n",
    "    def update_sc_reward(episode, reward):\n",
    "        with fig.batch_update():\n",
    "            fig.data[0].x += (episode,)\n",
    "            fig.data[0].y += (reward,)\n",
    "    \n",
    "    def sim(*args, **kwargs):\n",
    "        display(bt_stop)\n",
    "        display(fig)\n",
    "        for episode, reward in training(*args, **kwargs):\n",
    "            h_rewards.append(reward)\n",
    "            if episode % step_log == 0:\n",
    "                update_sc_reward(episode, np.mean(h_rewards))\n",
    "            if dashboard.STOP_TRAINING:\n",
    "                break\n",
    "        bt_stop.disabled = True\n",
    "    \n",
    "    def thread(*args, **kwargs):\n",
    "        t = threading.Thread(target=sim, args=args, kwargs=kwargs)\n",
    "        t.start()\n",
    "        return t\n",
    "    return thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38e6b8f3-0069-4657-bc65-2136db5553b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e347f180fd094740b5e08c4ef9390519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='danger', description='Stop Training', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Thread(Thread-5 (sim), started 139637483898560)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efeca0315c6d47b5b6514475a0a2d8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'markers+lines',\n",
       "              'type': 'scatter',\n",
       "              'uid': '36c8d038-8035-4a27-ad5a-b5e93ebfede2',\n",
       "              'x': [],\n",
       "              'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Dashboard for training RL agents'},\n",
       "               'xaxis': {'title': {'text': 'Episode'}},\n",
       "               'yaxis': {'rangemode': 'tozero', 'title': {'text': 'Reward'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dashboard\n",
    "def train_q_agent(Q, n_episodes):\n",
    "    \"\"\"\n",
    "    Го тренира агентот со Q-учење во средината MountainCar-v0.\n",
    "\n",
    "    Влезни аргументи:\n",
    "        Q (defaultdict): Q-табела што ги содржи Q-вредностите за секоја состојба и акција.\n",
    "        n_episodes (int): Број на епизоди за кои агентот ќе се тренира.\n",
    "\n",
    "    Опис:\n",
    "        - Во секоја епизода, околината се ресетира и агентот започнува од случајна почетна состојба.\n",
    "        - Се врши дискретизација на набљудувањето во состојба со `get_state_from_observation`.\n",
    "        - Додека епизодата не заврши (поради успех или истек на време), агентот избира акција според ε-greedy стратегија\n",
    "        - Се добива ново набљудување и награда, и Q-табелата се ажурира со `update_q`.\n",
    "        - По завршување на секоја епизода, се враќа бројот на епизодата и вкупната награда.\n",
    "\n",
    "    Излез:\n",
    "        Generator: Tuple `(episode, episode_reward)` за секоја епизода.\n",
    "    \"\"\"\n",
    "    with gym.make('MountainCar-v0') as env:\n",
    "        for episode in range(n_episodes):\n",
    "            observation, _ = env.reset()    \n",
    "            i_state = get_state_from_observation(observation)  \n",
    "            episode_reward = 0\n",
    "            terminated = truncated = False  \n",
    "            while not (terminated or truncated):\n",
    "                action = policy(env, i_state, Q, episode)\n",
    "                observation, reward, terminated, truncated, _ = env.step(action)  \n",
    "                i_next_state = get_state_from_observation(observation)\n",
    "                episode_reward += reward\n",
    "                update_q(Q, i_state, i_next_state, action, reward)\n",
    "                i_state = i_next_state\n",
    "            yield episode, episode_reward\n",
    "\n",
    "\n",
    "Q = defaultdict(lambda: defaultdict(int))\n",
    "train_q_agent(Q, n_episodes=100000)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6581953-05cf-4758-9d9a-77b8e692d73e",
   "metadata": {},
   "source": [
    "#### На графикот е прикажана вредноста на наградата што агентот ја добива во секоја епизода. Агентот добива награда -1 за секоја временска единица додека не го достигне знаменцето. На почетокот на тренирањето наградата изнесува -200, односно агентот не го достигнува знаменцето пред да заврши епизодата, епизодата за тренирање завршува после 200  временски единици. Како се зголемуваат епизодите, може да се увиде дека наградата расте, агентот го достигнува знаменцето, односно учи. Наградата достигнува стационарна вредност -162."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "148d6339-678e-460b-b5ae-0a20437f52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_q_agent(Q, n_episodes=1):\n",
    "    \"\"\"\n",
    "    Ја прикажува (визуелизира) работата на агентот кој е трениран со Q-учење во MountainCar околината.\n",
    "\n",
    "    Влезни аргументи:\n",
    "        Q (defaultdict): Q-табела со научените Q-вредности за секоја состојба и акција.\n",
    "        n_episodes (int): Број на епизоди што ќе се визуелизираат (стандардно 1).\n",
    "\n",
    "    Опис:\n",
    "        - За секоја епизода, агентот почнува од иницијална состојба.\n",
    "        - Ако не постои запишана акција за дадената состојба, се избира случајна акција.\n",
    "        - Ако има достапни Q-вредности, се избира акцијата со највисока вредност (експлоатација).\n",
    "        - Процесот продолжува сè додека не заврши епизодата (успешно или со истекување на времето).\n",
    "    \"\"\"\n",
    "    with gym.make('MountainCar-v0', render_mode='human') as env:\n",
    "        for episode in range(n_episodes):\n",
    "            state, _ = env.reset()\n",
    "            i_state = get_state_from_observation(state)\n",
    "            terminated = truncated = False\n",
    "            while not (terminated or truncated):\n",
    "                if not Q[i_state]:\n",
    "                    action = env.action_space.sample()        \n",
    "                else:\n",
    "                    action = max(Q[i_state], key=Q[i_state].get)    \n",
    "                state, _, terminated, truncated, _ = env.step(action)\n",
    "                i_state = get_state_from_observation(state)\n",
    "\n",
    "\n",
    "visualise_q_agent(Q, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa62b1-7b8b-43a1-b382-483ea4bd1813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
